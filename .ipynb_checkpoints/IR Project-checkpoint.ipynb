{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d6d3084",
   "metadata": {
    "id": "4d6d3084"
   },
   "outputs": [],
   "source": [
    "#Importing all the required modules\n",
    "import nltk\n",
    "import os\n",
    "from os import walk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.metrics.distance  import edit_distance\n",
    "from nltk.corpus import words\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d3b37ca",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3d3b37ca",
    "outputId": "62751d71-2096-446c-9965-f305abc690e8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Abhisht\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Abhisht\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Abhisht\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Downloading the corpus required\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65c3764a",
   "metadata": {
    "id": "65c3764a"
   },
   "outputs": [],
   "source": [
    "#For Stemming we are using Snowball Stemmer, which is basically a improved porter stemmer.\n",
    "snow_stemmer = SnowballStemmer(language='english')\n",
    "\n",
    "#To-complete, for Spelling correction method.\n",
    "correct_words = words.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b54e9c2",
   "metadata": {
    "id": "1b54e9c2"
   },
   "outputs": [],
   "source": [
    "## global dictionary with content of each Document after stopword removal and lemmatization\n",
    "file_content={}\n",
    "# Word - Document inverted index\n",
    "word_doc={}\n",
    "#Bigram index\n",
    "bigram_doc = {}\n",
    "\n",
    "#Universal Set\n",
    "universal_set=set()\n",
    "\n",
    "#Global Dictionary of file index\n",
    "file_index={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97Lw1dmHCEcJ",
   "metadata": {
    "id": "97Lw1dmHCEcJ"
   },
   "outputs": [],
   "source": [
    "def stopword_removal(wordset):\n",
    "    \"\"\"\n",
    "    ### Description:\n",
    "    removes english stopwords from a tokenized list\n",
    "    \n",
    "    ### Args:\n",
    "    `wordset`:list which contains one english word as one element\n",
    "    \n",
    "    ### Returns:\n",
    "    returns a list in which doesn't contains english stopwords\n",
    "    \"\"\"\n",
    "    stops = set(stopwords.words('english'))\n",
    "    new_wordset = [word for word in wordset if not word in stops]\n",
    "    return new_wordset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "iQufno3lCDYw",
   "metadata": {
    "id": "iQufno3lCDYw"
   },
   "outputs": [],
   "source": [
    "def stemming(wordset):\n",
    "    \"\"\"\n",
    "    ### Description:\n",
    "    performs stemming on each word of the tokenized list\n",
    "    \n",
    "    ### Args:\n",
    "    `wordset`:list which contains one english word as one element\n",
    "    \n",
    "    ### Returns:\n",
    "    returns a list in which each element of the list stemmed\n",
    "    \"\"\"\n",
    "    stemmed_wordset = set()\n",
    "    for word in wordset:\n",
    "        stemmed_wordset.add(snow_stemmer.stem(word))\n",
    "    return stemmed_wordset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbef4a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def addToBigramIndex(word):\n",
    "    \"\"\"\n",
    "    ### Description:\n",
    "    bigram_doc maintains an inverted index from bigrams to the terms that contain the bigram\n",
    "    This method adds element to a bigram_doc with bigram as keys and set of words containing that bigram as the value\n",
    "    \n",
    "    ### Args:\n",
    "    `word`:Word whose bigrams need to be added to the bigram_doc\n",
    "    \n",
    "    ### Returns:\n",
    "    Doesn't return anything but updates the bigram_doc\n",
    "    \"\"\"\n",
    "    orig = word\n",
    "    word = \"$\" + word + \"$\"\n",
    "    for i in range(0,len(word)-1):\n",
    "        bigram = word[i:i+2]\n",
    "        if bigram in bigram_doc.keys():\n",
    "            bigram_doc[bigram].add(orig)\n",
    "        else:\n",
    "            bigram_doc[bigram]=set()\n",
    "            bigram_doc[bigram].add(orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce411351",
   "metadata": {
    "id": "ce411351"
   },
   "outputs": [],
   "source": [
    "## ORDER OF OPERATIONS -\n",
    "# 1)Tokenization\n",
    "# 2)Stopword Removal\n",
    "# 3)Stemming\n",
    "\n",
    "directory= \"dataset\"\n",
    "file_list = os.listdir(directory)\n",
    "file_index = {i+1:file_list[i] for i in range(len(file_list))}\n",
    "universal_set={i+1 for i in range(len(file_list))}\n",
    "for i in range(1,len(file_index)+1):\n",
    "    wordset=set()\n",
    "    with open(directory+'/'+file_index[i],'r') as file:\n",
    "\n",
    "        ##Tokenization Begins\n",
    "        for line in file:\n",
    "            text_tokens = word_tokenize(line)\n",
    "            text_tokens_lower = [word.lower() for word in text_tokens]\n",
    "            wordset.update(text_tokens_lower)\n",
    "        ##Tokenization Ends \n",
    "    ##Stopword Removal Begins\n",
    "    temp_list = stopword_removal(wordset)\n",
    "    ##Stopword Removal Ends\n",
    "\n",
    "    ## Stemming Begins\n",
    "    new_wordset = stemming(temp_list)\n",
    "    ## Stemming Ends\n",
    "\n",
    "    file_content[i] = new_wordset\n",
    "\n",
    "for i in range(1,len(file_index)+1):\n",
    "    for word in file_content[i]:\n",
    "        if word in word_doc.keys():\n",
    "            word_doc[word].add(i)\n",
    "        else:\n",
    "            word_doc[word]=set()\n",
    "            word_doc[word].add(i)\n",
    "            addToBigramIndex(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53cfe01a",
   "metadata": {
    "id": "53cfe01a"
   },
   "outputs": [],
   "source": [
    "# Returns the list of documents name when we are provided with a list of indexes\n",
    "def doc_name(list1):\n",
    "    \"\"\"\n",
    "    ### Description:\n",
    "    # Returns the list of documents name when we are provided with a list of indexes\n",
    "    \"\"\"\n",
    "    docs=[]\n",
    "    for k in list1:\n",
    "        docs.append(file_index[k])\n",
    "        \n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01cf37f8",
   "metadata": {
    "id": "01cf37f8"
   },
   "outputs": [],
   "source": [
    "# Intersection of two arrays (used in AND)\n",
    "def intersection(list1, list2):\n",
    "    \"\"\"\n",
    "    ### Description:\n",
    "    Intersection of two arrays (used in AND)\n",
    "    \"\"\"\n",
    "    list3 = [value for value in list1 if value in list2]\n",
    "    return set(list3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4648e76",
   "metadata": {
    "id": "d4648e76"
   },
   "outputs": [],
   "source": [
    "def union(list1,list2):\n",
    "    \"\"\"\n",
    "    ### Description:\n",
    "    Union of two arrays (used in OR)\n",
    "    \"\"\"\n",
    "    temp_set=list1\n",
    "    temp_set.update(list2)\n",
    "    return temp_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c042fcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc(word):\n",
    "    \"\"\"\n",
    "    ### Description:\n",
    "    This method return a list of documents which contain that word or that wildcard query. If its a wildcard query then documentis returned after postfiltering using regex \n",
    "    \n",
    "    ### Args:\n",
    "    `word`: whose document list needs to be returned\n",
    "    \n",
    "    ### Returns:\n",
    "    document list containing that word or wildcard query is returned back\n",
    "    \"\"\"\n",
    "    if '*' in word:\n",
    "        if word[0]!='*':\n",
    "            word='$'+word\n",
    "            \n",
    "        if word[-1]!='*':\n",
    "            word=word+'$'\n",
    "        \n",
    "        intersection_list=set()\n",
    "        flag=1\n",
    "        for i in range(0,len(word)-1):\n",
    "            if word[i]!='*' and word[i+1]!='*':\n",
    "                temp_bigram=word[i:i+2]\n",
    "                if temp_bigram in bigram_doc:\n",
    "                    temp_list=list(bigram_doc[temp_bigram])#this will give you the list of all the word matching that pattern\n",
    "                    if flag==1:\n",
    "                        intersection_list=temp_list\n",
    "                        flag=0\n",
    "                    else:\n",
    "                        intersection_list=intersection(intersection_list, temp_list)# will contain the final words and now we will take their union\n",
    "        \n",
    "        postfiltering_list=[]\n",
    "        \n",
    "        #Post Filtering with Regex\n",
    "        regex_pattern=\"\"\n",
    "        if word[0]=='$':\n",
    "            regex_pattern+=\"^\"\n",
    "        else:\n",
    "            regex_pattern+=\".*\"\n",
    "        \n",
    "        for i in range(1,len(word)-1):\n",
    "            if word[i]=='*':\n",
    "                regex_pattern+='.'\n",
    "            else:\n",
    "                regex_pattern+=word[i]\n",
    "        \n",
    "        if word[-1]=='$':\n",
    "            regex_pattern+=\"$\"\n",
    "        else:\n",
    "            regex_pattern+=\".*\"\n",
    "        \n",
    "        pattern=re.compile(regex_pattern)\n",
    "        for word1 in intersection_list:\n",
    "            if re.search(regex_pattern, word1):\n",
    "                postfiltering_list.append(word1)\n",
    "        \n",
    "        final_list=set()\n",
    "        for word1 in postfiltering_list:\n",
    "            if word1 in word_doc:\n",
    "                #print(word1)\n",
    "                final_list=union(final_list,word_doc[word1])\n",
    "                               \n",
    "        return final_list\n",
    "    elif word in word_doc:\n",
    "        return word_doc[word]\n",
    "    else:\n",
    "        return set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9236e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method return the precedence of boolean operators\n",
    "def precedence(op):\n",
    "    \"\"\"\n",
    "    ### Description:\n",
    "    This method return the precedence of boolean operators\n",
    "    \"\"\"\n",
    "    if op == 'or':\n",
    "        return 1   \n",
    "    if op == 'and':\n",
    "        return 2\n",
    "    if op == 'not':\n",
    "        return 3\n",
    "    return 0\n",
    "\n",
    "# This method applies boolean operation on the sub-query\n",
    "def applyOp(a, b, op):\n",
    "    \"\"\"\n",
    "    ### Description:\n",
    "    This method applies boolean operation on the sub-query\n",
    "    \"\"\"\n",
    "    if op == 'and': return a.intersection(b)\n",
    "    if op == 'or': return a.union(b)\n",
    "    if op == 'not': return universal_set - a\n",
    "\n",
    "    \n",
    "#This Method parses the provided query and returns the resultant list\n",
    "def query_parser(query):\n",
    "    \"\"\"\n",
    "    ### Description:\n",
    "    This Method parses the provided query and returns the resultant list\n",
    "    \"\"\"\n",
    "    query = \"(\" + query.lower() + \")\"\n",
    "    expression = query.split()\n",
    "    temp_list=[]\n",
    "    for token in expression:\n",
    "        i=0\n",
    "        size = len(token)\n",
    "        while (i < size and token[i]==\"(\"):\n",
    "            temp_list.append(\"(\")\n",
    "            i = i + 1\n",
    "        temp_word = \"\"\n",
    "        while (i < size and token[i]!=\")\"):\n",
    "            temp_word = temp_word + token[i]\n",
    "            i = i + 1\n",
    "        temp_list.append(temp_word)\n",
    "        while (i < size and token[i]==\")\"):\n",
    "            temp_list.append(\")\")\n",
    "            i = i + 1\n",
    "    return temp_list\n",
    "\n",
    "#This method evaluates the query provided by the user\n",
    "def evaluate(tokens):\n",
    "    \"\"\"\n",
    "    ### Description:\n",
    "    This method evaluates the query provided by the user\n",
    "    \"\"\"\n",
    "    values = []\n",
    "    ops = []\n",
    "    i = 0 \n",
    "    while i < len(tokens):\n",
    "         \n",
    "        if tokens[i] == '(':\n",
    "            ops.append(tokens[i])\n",
    "\n",
    "        elif tokens[i] == ')':\n",
    "\n",
    "            while len(ops) != 0 and ops[-1] != '(': \n",
    "                op = ops.pop() \n",
    "                if(op=='not'):\n",
    "\n",
    "                    val1 = values.pop()\n",
    "                    values.append(applyOp(val1,\"\",op))\n",
    "\n",
    "                else: \n",
    "\n",
    "                    val1 = values.pop()            \n",
    "                    val2 = values.pop()\n",
    "                    values.append(applyOp(val1, val2, op))\n",
    "\n",
    "            ops.pop()\n",
    "         \n",
    "        # Current token is an operator.\n",
    "        elif tokens[i]=='and' or tokens[i]=='or' or tokens[i]=='not':\n",
    "\n",
    "            while (len(ops) != 0 and precedence(ops[-1]) >= precedence(tokens[i])):\n",
    "\n",
    "                op = ops.pop()\n",
    "                if(op=='not'):\n",
    "\n",
    "                    val1 = values.pop()\n",
    "                    values.append(applyOp(val1,\"\",op))\n",
    "\n",
    "                else: \n",
    "\n",
    "                    val1 = values.pop()            \n",
    "                    val2 = values.pop()\n",
    "                    values.append(applyOp(val1, val2, op))\n",
    "             \n",
    "            ops.append(tokens[i])\n",
    "\n",
    "        else:\n",
    "            \n",
    "            values.append(get_doc(snow_stemmer.stem(tokens[i])))\n",
    " \n",
    "        i += 1\n",
    "\n",
    "    while len(ops) != 0:\n",
    "        op = ops.pop()\n",
    "        if(op=='not'):\n",
    "\n",
    "            val1 = values.pop()\n",
    "            values.append(val1,\"\",op)\n",
    "\n",
    "        else: \n",
    "\n",
    "            val1 = values.pop()            \n",
    "            val2 = values.pop()\n",
    "            values.append(applyOp(val1, val2, op))\n",
    "     \n",
    "    return values[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b23da77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some Examples of querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19e28413",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['venus-and-adonis_TXT_FolgerShakespeare.txt']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Example 1\n",
    "#This would give you all the documents where mon* is not present in the document OR *beam is present in the document.\n",
    "doc_name(evaluate(query_parser(\"not (mon*)\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4db3f075",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['antony-and-cleopatra_TXT_FolgerShakespeare.txt']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This would show you all the documents where 'cleopatra' and 'brutus' Occurs Together.\n",
    "doc_name(evaluate(query_parser(\"cleopatra and brutus\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7884405",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['romeo-and-juliet_TXT_FolgerShakespeare.txt',\n",
       " 'measure-for-measure_TXT_FolgerShakespeare.txt']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This would give you all documents name where it matches the wildcard query 'juli*t'.\n",
    "doc_name(get_doc('juli*t'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "652f06ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some More Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ed9c19bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['romeo-and-juliet_TXT_FolgerShakespeare.txt',\n",
       " 'as-you-like-it_TXT_FolgerShakespeare.txt',\n",
       " 'cymbeline_TXT_FolgerShakespeare.txt']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_name(evaluate(query_parser(\"cleopatra and not (brutus)\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cc8366c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pericles_TXT_FolgerShakespeare.txt',\n",
       " 'romeo-and-juliet_TXT_FolgerShakespeare.txt',\n",
       " 'the-winters-tale_TXT_FolgerShakespeare.txt',\n",
       " 'cymbeline_TXT_FolgerShakespeare.txt']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_name(evaluate(query_parser(\"cleo* and not (brut*)\")))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Code.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
